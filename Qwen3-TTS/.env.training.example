# DATA MODE - How to load training data
DATA_MODE=jsonl

# MODEL PATHS - Locations of models and tokenizer
INIT_MODEL_PATH=Qwen/Qwen3-TTS-12Hz-1.7B-Base
TOKENIZER_PATH=Qwen/Qwen3-TTS-Tokenizer-12Hz
OUTPUT_DIR=./output
SPEAKER_NAME=hausa_speaker

# DATA FILES - Paths to training and validation JSONL files
TRAIN_JSONL=./data/train.jsonl
VALIDATION_JSONL=./data/validation.jsonl

# HuggingFace dataset name (used for direct mode)
DATASET_NAME=vaghawan/hausa-tts-22k

# TRAINING HYPERPARAMETERS - Learning rate, epochs, batch size
BATCH_SIZE=4
LEARNING_RATE=5e-4
NUM_EPOCHS=3
GRADIENT_ACCUMULATION_STEPS=1
WEIGHT_DECAY=0.01
WARMUP_STEPS=200
MAX_GRAD_NORM=1.0
SUB_TALKER_LOSS_WEIGHT=0.3

# LAYER REPLACEMENT - How to modify model architecture
REPLACE_LAST_N_LAYERS=1
ADD_NEW_LAYERS=1
FREEZE_ORIGINAL_LAYERS=true
FREEZE_SPEAKER_ENCODER=true

# LOGGING AND CHECKPOINTING - Frequency of operations
LOGGING_STEPS=50
SAVE_STEPS=500
EVAL_STEPS=500
SAVE_TOTAL_LIMIT=2

# WANDB CONFIGURATION - Experiment tracking and metrics
USE_WANDB=true
WANDB_PROJECT=qwen3-tts-training
WANDB_RUN_NAME=experiment-feb-24
WANDB_ENTITY=kristina-ghimire-ai-none
WANDB_API_KEY=wandb_v1_*************

# HUGGINGFACE CONFIGURATION - Model upload settings
UPLOAD_TO_HF=true
HF_TOKEN=hf_*************
HF_BEST_MODEL_REPO=vaghawan/tts-best-checkpoint
HF_LAST_MODEL_REPO=vaghawan/tts-last-checkpoint

# DEVICE AND PRECISION - Training device setup
DEVICE=cuda
MIXED_PRECISION=no

# DATA LIMITS - Max samples for testing/debugging
MAX_TRAIN_SAMPLES=3000
MAX_EVAL_SAMPLES=300
