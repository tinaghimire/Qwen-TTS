# DATA MODE - How to load training data
# jsonl: Uses pre-prepared JSONL files (best for small/medium datasets <100k samples)
# direct: Streams directly from HuggingFace (required for large datasets >100k samples)
# none: Skips data preparation (for testing/debugging)
DATA_MODE=direct

# CRITICAL: For multi-GPU training with large datasets (600k+ samples), use:
# DATA_MODE=direct
# BATCH_SIZE=8  (per GPU)
# LEARNING_RATE=1e-3  (scaled for multi-GPU)
# WARMUP_STEPS=1000
# MIXED_PRECISION=bf16

# MODEL PATHS
INIT_MODEL_PATH=Qwen/Qwen3-TTS-12Hz-1.7B-Base
TOKENIZER_PATH=Qwen/Qwen3-TTS-Tokenizer-12Hz
OUTPUT_DIR=./output
SPEAKER_NAME=hausa_speaker

# DATA FILES
TRAIN_JSONL=./data/train.jsonl
VALIDATION_JSONL=./data/validation.jsonl

# TRAINING HYPERPARAMETERS
BATCH_SIZE=64
LEARNING_RATE=3e-4
NUM_EPOCHS=5
GRADIENT_ACCUMULATION_STEPS=1
WEIGHT_DECAY=0.01
WARMUP_STEPS=10
MAX_GRAD_NORM=1.0
SUB_TALKER_LOSS_WEIGHT=0.3
AUDIO_LOSS_EVERY_N_STEPS=5

# LAYER REPLACEMENT
REPLACE_LAST_N_LAYERS=0
ADD_NEW_LAYERS=0
FREEZE_ORIGINAL_LAYERS=false
FREEZE_SPEAKER_ENCODER=false

# LOGGING AND CHECKPOINTING
LOGGING_STEPS=1
SAVE_STEPS=5
EVAL_STEPS=5
SAVE_TOTAL_LIMIT=2

# WANDB CONFIGURATION
USE_WANDB=true
WANDB_PROJECT=qwen3-tts-training
WANDB_RUN_NAME=70k-samples-training

# HUGGINGFACE CONFIGURATION
UPLOAD_TO_HF=true
HF_BEST_MODEL_REPO=vaghawan/tts-600k-best
HF_LAST_MODEL_REPO=vaghawan/tts-600k-last

# DEVICE AND PRECISION
DEVICE=cuda
MIXED_PRECISION=bf16

# DATA LIMITS
MAX_TRAIN_SAMPLES=150000
MAX_VAL_SAMPLES=10000

# # For data proceassing in background
# DATA_MODE=streaming
# BATCH_SIZE=8
# GRADIENT_ACCUMULATION_STEPS=2
# LEARNING_RATE=2e-4

# AUXILIARY LOSSES
USE_AUXILIARY_LOSSES=true
MEL_RECONSTRUCTION_WEIGHT=0.2
RECONSTRUCTION_WEIGHT=0.15
VOICE_CONSISTENCY_WEIGHT=0.2
PROSODY_WEIGHT=0.15
