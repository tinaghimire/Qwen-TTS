# DATA MODE - How to load training data
# jsonl: Uses pre-prepared JSONL files (best for small/medium datasets <100k samples)
# direct: Streams directly from HuggingFace (required for large datasets >100k samples)
# none: Skips data preparation (for testing/debugging)
DATA_MODE=direct

# CRITICAL: For multi-GPU training with large datasets (600k+ samples), use:
# DATA_MODE=direct
# BATCH_SIZE=8  (per GPU)
# LEARNING_RATE=1e-3  (scaled for multi-GPU)
# WARMUP_STEPS=1000
# MIXED_PRECISION=bf16

# MODEL PATHS
INIT_MODEL_PATH=Qwen/Qwen3-TTS-12Hz-1.7B-Base
TOKENIZER_PATH=Qwen/Qwen3-TTS-Tokenizer-12Hz
OUTPUT_DIR=./output
SPEAKER_NAME=hausa_speaker

# DATA FILES
TRAIN_JSONL=./data/train.jsonl
VALIDATION_JSONL=./data/validation.jsonl

# HF MULTI-SPEAKER DATA (used by finetune.py)
# Dataset repo and configs: one config per speaker; ref audio from voices/{speaker}.wav
HF_DATASET_REPO=vaghawan/qwen3-tts-multi-speaker
TRAIN_SPEAKERS=hausa_speaker,english_speaker

# TRAINING HYPERPARAMETERS
# For 70+ GB GPU (e.g. A100 80GB): TRAIN_BATCH_SIZE=128, VALIDATION_BATCH_SIZE=256, GRADIENT_ACCUMULATION_STEPS=1
# For 24 GB GPU: TRAIN_BATCH_SIZE=8–16, GRADIENT_ACCUMULATION_STEPS=4–8
# How long to train: 3–5 epochs with 150k samples (~1.2k steps/epoch at batch 128); monitor val loss and stop if it plateaus.
BATCH_SIZE=64
LEARNING_RATE=3e-4
NUM_EPOCHS=5
GRADIENT_ACCUMULATION_STEPS=1
WEIGHT_DECAY=0.01
WARMUP_STEPS=10
MAX_GRAD_NORM=1.0
SUB_TALKER_LOSS_WEIGHT=0.3
AUDIO_LOSS_EVERY_N_STEPS=5

# LAYER REPLACEMENT
REPLACE_LAST_N_LAYERS=0
ADD_NEW_LAYERS=0
FREEZE_ORIGINAL_LAYERS=false
FREEZE_SPEAKER_ENCODER=false

# LOGGING AND CHECKPOINTING
LOGGING_STEPS=1
SAVE_STEPS=5
EVAL_STEPS=5
SAVE_TOTAL_LIMIT=2

# WANDB CONFIGURATION
USE_WANDB=true
WANDB_PROJECT=qwen3-tts-training
WANDB_RUN_NAME=70k-samples-training

# HUGGINGFACE CONFIGURATION
UPLOAD_TO_HF=true
HF_BEST_MODEL_REPO=vaghawan/tts-600k-best
HF_LAST_MODEL_REPO=vaghawan/tts-600k-last

# DEVICE AND PRECISION
DEVICE=cuda
MIXED_PRECISION=bf16

# MODEL LOADING AND MAX GPU USE
# Load model directly on GPU from the start (recommended for single-GPU to maximize GPU use).
# For 70+ GB GPU the full model fits on device; no CPU offload needed.
# Examples: cuda:0 (first GPU), cuda (default GPU). Leave empty for multi-GPU / let Accelerator place.
# MODEL_DEVICE_MAP=cuda:0

# DATALOADER: CPU workers and prefetch to keep GPU fed
# num_workers: Unset = auto from GPU/CPU count. For streaming, capped at 4.
# DATALOADER_NUM_WORKERS=16
# DATALOADER_PREFETCH_FACTOR=8
# DATALOADER_PERSISTENT_WORKERS=true
# Tokenizer/audio encoding: false = use GPU in workers (faster); true = use CPU (saves VRAM, use if OOM).
USE_CPU_FOR_TOKENIZER_AUDIO_CODES=false
# Only speech_tokenizer.decoder on CPU (encoder on GPU). Saves VRAM; decode() moves data to CPU and back.
# SPEECH_TOKENIZER_DECODER_ON_CPU=true

# DATA LIMITS
# Cap train/val size (dataset has ~153k train + 10k val for hausa+english). Unset = use full dataset.
MAX_TRAIN_SAMPLES=150000
MAX_VAL_SAMPLES=10000

# STREAMING (low RAM): stream from HuggingFace instead of loading full split into memory
# USE_STREAMING_DATASET=true
# SHUFFLE_BUFFER_SIZE=1000

# # For data proceassing in background
# DATA_MODE=streaming
# BATCH_SIZE=8
# GRADIENT_ACCUMULATION_STEPS=2
# LEARNING_RATE=2e-4

# AUXILIARY LOSSES
USE_AUXILIARY_LOSSES=true
MEL_RECONSTRUCTION_WEIGHT=0.2
RECONSTRUCTION_WEIGHT=0.15
VOICE_CONSISTENCY_WEIGHT=0.2
PROSODY_WEIGHT=0.15
